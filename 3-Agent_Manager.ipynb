{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Administrador de agentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código permite la creación de un sistema para gestionar y coordinar diferentes agentes. En el ejemplo se muestra la capacidad del coordinador para escoger el mejor agente para realizar una tarea especifica.\n",
    "\n",
    "Esencialmente, `AgentManager` actúa como un coordinador, interpretando los mensajes de los usuarios, eligiendo el agente más adecuado para responder a esa solicitud, y pasando el mensaje a ese agente para su procesamiento. También gestiona cualquier interacción subsiguiente entre los agentes y el usuario, manejando la memoria compartida del diálogo para mantener un contexto coherente durante la conversación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Importaciones y configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "from exec_code import execute_code, extract_code_from_string\n",
    "from prompt import CODE_DEVELOPER_SYSTEMPROMPT, DOCUMENTATION_DEVELOPER_SYSTEMPROMPT, COORDINATOR_SYSTEMPROMPT\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Definición de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "    \n",
    "    def __init__(self, \n",
    "        model: Optional[str] = None, \n",
    "        system_prompt: Optional[str] = None\n",
    "    ) -> None:\n",
    "        \n",
    "        model = model if model else os.getenv(\"OPENAI_DEFAULT_MODEL\")\n",
    "        \n",
    "        self.memory = []\n",
    "        self.model = model\n",
    "        \n",
    "        if system_prompt:\n",
    "            self.memory.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    def add_to_memory(self, role, message):\n",
    "        self.memory.append({\"role\": role, \"content\": message})\n",
    "\n",
    "    def get_schema(self):\n",
    "        return self.function\n",
    "        \n",
    "    @abstractmethod\n",
    "    def run(self,prompt):\n",
    "        \"\"\"User must define this method. Run the agent\"\"\"\n",
    "    \n",
    "class ConversationAgent(Agent):\n",
    "    \n",
    "    def __init__(self, \n",
    "        model: Optional[str] = None, \n",
    "        system_prompt: Optional[str] = None,\n",
    "        agent_name: Optional[str] = None,\n",
    "        description: Optional[str] = None\n",
    "    ) -> None:\n",
    "        super().__init__(model,system_prompt)\n",
    "\n",
    "        self.agent_name = agent_name\n",
    "        self.description = description\n",
    "\n",
    "        self.function={\n",
    "            \"name\": self.agent_name,\n",
    "            \"description\": self.description,\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"user_prompt\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"User prompt to the agent\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"user_query\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def run(self,prompt):\n",
    "\n",
    "        self.add_to_memory(\"user\", prompt)\n",
    "        completion=''\n",
    "        stream = client.chat.completions.create(\n",
    "            messages=self.memory,\n",
    "            model=self.model,\n",
    "            stream=True\n",
    "        )\n",
    "        print(f\"\\n{self.agent_name}:\")\n",
    "        for chunk in stream:\n",
    "            text_chunk=chunk.choices[0].delta.content\n",
    "            if text_chunk:\n",
    "                completion+=text_chunk\n",
    "                print(text_chunk, end='', flush=True)\n",
    "        \n",
    "        self.add_to_memory(\"assistant\", completion)\n",
    "        \n",
    "        return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Configuración del AgentManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El `AgentManager` es una clase que hereda de la clase abstracta `Agent`. Esta clase está diseñada para orquestar y manejar las interacciones entre varios agentes conversacionales, como `CodeDeveloper`, `ImproveDeveloper` y `DocumentationDeveloper`. Cada uno de estos agentes tiene funciones especializadas, como generación de código, diseño de interfaces de usuario, y creación de documentación, respectivamente.\n",
    "\n",
    "Aquí están los elementos clave de la clase `AgentManager`:\n",
    "\n",
    "- **run**: El método `run` es donde se lleva a cabo la lógica principal del `AgentManager`. Cuando se llama a `run` con un mensaje de usuario:\n",
    "  1. Añade el mensaje a la memoria del `AgentManager`.\n",
    "  2. Decide cuál de los agentes debe manejar esta solicitud basándose en su comprensión del mensaje y la lista de agentes disponibles.\n",
    "  3. Basándose en esta información, selecciona el agente adecuado y le pasa la solicitud.\n",
    "  4. El método continúa y ejecuta el agente seleccionado, esperando su respuesta. Si la respuesta contiene código para ejecutar, intenta ejecutar este código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AgentManager(Agent):\n",
    "    \n",
    "    def __init__(self, \n",
    "        model: Optional[str] = None, \n",
    "        system_prompt: Optional[str] = None,\n",
    "        list_agents: Optional[list] = None\n",
    "    ) -> None:\n",
    "        super().__init__(model,system_prompt)\n",
    "        \n",
    "        self.list_agents=list_agents\n",
    "    \n",
    "    \n",
    "    def get_agent_list_schemas(self):\n",
    "        return [agent.get_schema() for agent in self.list_agents]   \n",
    "    \n",
    "    def run(self,prompt):\n",
    "        \n",
    "        print(f\"----------------- RUN MANAGER -----------------\")\n",
    "        \n",
    "        # Add the prompt to the agent manager memory and the others agents\n",
    "        ######################################\n",
    "        self.add_to_memory(\"user\", prompt)\n",
    "        print(f\"User: {prompt}\")\n",
    "        \n",
    "        # add memory to others agents\n",
    "        for agent in self.list_agents:\n",
    "            agent.add_to_memory(\"user\", agent_prompt)\n",
    "                    \n",
    "        # Choose what agent i have to execute to resolve the problem (Select speaker)\n",
    "        ######################################\n",
    "        messages = self.memory + [{\"role\": \"user\", \"content\": f\"[PROMPT]\\n{prompt}\\n\\n[AGENTS]\\n{self.get_agent_list_schemas()}\"}]\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=self.model,\n",
    "            functions=self.get_agent_list_schemas()\n",
    "        )\n",
    "                \n",
    "        # Get the agent to execute\n",
    "        agent_name=chat_completion.choices[0].message.function_call.name\n",
    "        agent_prompt=json.loads(chat_completion.choices[0].message.function_call.arguments)['user_prompt']\n",
    "         \n",
    "        #get the class agent to execute\n",
    "        class_agent = next((agent for agent in self.list_agents if agent.agent_name == agent_name), None)\n",
    "        \n",
    "        # Save agents and remove the agent choosen to another list. It is use to store memory in the future in all agents without the agent choosen\n",
    "        other_agents = [agent for agent in self.list_agents if agent.agent_name != agent_name]\n",
    "        \n",
    "        #Execute the algorithm\n",
    "        ######################################\n",
    "        for i in range(3):\n",
    "            \n",
    "            # # Run agent\n",
    "            ######################################\n",
    "            response_agent=class_agent.run(f\"{class_agent.agent_name}: {agent_prompt}\")\n",
    "            # add memory to others agents\n",
    "            for agent in other_agents:\n",
    "                agent.add_to_memory(\"user\", f\"{class_agent.agent_name}: {agent_prompt}\")\n",
    "            \n",
    "            # if we have code to execute, execute it\n",
    "            ######################################\n",
    "            code_to_execute = extract_code_from_string(response_agent)\n",
    "            if code_to_execute:\n",
    "                try:\n",
    "                    print(\"Executing code...\")\n",
    "                    exec_response = execute_code(code_to_execute,use_docker=False)\n",
    "                    if exec_response[0] == 0:\n",
    "                        print(f\"Code executed successfully\")\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"The code: {code_to_execute}.\\n\\nError executing code: {exec_response[1]}\")\n",
    "                        agent_prompt=f\"\\n\\nError executing code: {exec_response[1]}\"\n",
    "                except Exception as e:\n",
    "                    print(f\"The code: {code_to_execute}.\\n\\nError executing code: {e}\")\n",
    "                    agent_prompt=f\"\\n\\nError executing code: {e}\"\n",
    "            else:\n",
    "                print(f\"No code found to execute.\")\n",
    "                agent_prompt=f\"\\n\\nNo code found to execute.\"\n",
    "        \n",
    "        # Add the response from the agent manager to the memory\n",
    "        ######################################\n",
    "        self.add_to_memory(\"assistant\", response_agent)\n",
    "                \n",
    "        return response_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 - Configuración de los agentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tres agentes, `CodeDeveloper`, `ImproveDeveloper` y `DocumentationDeveloper`, son subclases especializadas de `ConversationAgent`, cada una equipada para manejar diferentes tipos de solicitudes mediante el `system_prompt` que se haya configurado:\n",
    "\n",
    "- **CodeDeveloper**: Genera código en respuesta a problemas o solicitudes de programación proporcionados por el usuario. Su propósito es interactuar con los usuarios que necesitan asistencia en la creación de scripts o software.\n",
    "\n",
    "- **ImproveDeveloper**: Atiende solicitudes relacionadas con el el mejoramiento diseño. Este agente ofrece soluciones o propuestas de diseño para que las interfaces sean más efectivas y atractivas.\n",
    "\n",
    "- **DocumentationDeveloper**: Se especializa en la creación de documentación para código o proyectos de software. Este agente ayuda a los usuarios a redactar una documentación clara y comprensible, importante para cualquier proyecto de desarrollo.\n",
    "\n",
    "- **AgentManager**: Es el coordinador central que gestiona qué agente, desde su listado de agentes, debe responder a una solicitud específica. Interpreta los mensajes del usuario, selecciona al agente más adecuado basándose en la naturaleza del mensaje y la especialización del agente, y redirige la solicitud a ese agente para su procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CodeDeveloper=ConversationAgent(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    agent_name=\"CodeDeveloper\",\n",
    "    description=\"Agent to create code in python\",\n",
    "    system_prompt=CODE_DEVELOPER_SYSTEMPROMPT\n",
    ")\n",
    "\n",
    "ImproveDeveloper=ConversationAgent(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    agent_name=\"ImproveDeveloper\",\n",
    "    description=\"Agent to resolve improvements in python\",\n",
    "    system_prompt=CODE_DEVELOPER_SYSTEMPROMPT\n",
    ")\n",
    "\n",
    "DocumentationDeveloper=ConversationAgent(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    agent_name=\"DocumentationDeveloper\",\n",
    "    description=\"Agent to document code\",\n",
    "    system_prompt=DOCUMENTATION_DEVELOPER_SYSTEMPROMPT\n",
    ")\n",
    "\n",
    "# Manager Agents\n",
    "manager=AgentManager(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    list_agents=[\n",
    "        CodeDeveloper,\n",
    "        DocumentationDeveloper,\n",
    "        ImproveDeveloper\n",
    "    ],\n",
    "    system_prompt=COORDINATOR_SYSTEMPROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5- Ejecutar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- RUN MANAGER -----------------\n",
      "User: Create a code for a face detector using my camera, show me it in display and it must run by 30 sec and end.\n",
      "Manager: Create a Python program that uses the camera to detect faces, displays the output, and terminates after 30 seconds.\n",
      "\n",
      "CodeDeveloper:\n",
      "Below is a Python script using OpenCV to detect faces from the camera feed, display the output and terminate after 30 seconds:\n",
      "\n",
      "```python\n",
      "import cv2\n",
      "import time\n",
      "\n",
      "# Load a pre-trained face detection model\n",
      "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
      "\n",
      "# Initialize the video capture object\n",
      "cap = cv2.VideoCapture(0)\n",
      "\n",
      "# Check if the camera opened successfully\n",
      "if not cap.isOpened():\n",
      "    print(\"Cannot open camera\")\n",
      "    exit()\n",
      "\n",
      "start_time = time.time()\n",
      "\n",
      "while True:\n",
      "    # Read a frame from the camera\n",
      "    ret, frame = cap.read()\n",
      "    \n",
      "    # If frame is read correctly, ret is True\n",
      "    if not ret:\n",
      "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
      "        break\n",
      "\n",
      "    # Convert the frame to grayscale (face detection is more efficient in grayscale)\n",
      "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "    # Detect faces in the frame\n",
      "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
      "\n",
      "    # Draw rectangles around the faces\n",
      "    for (x, y, w, h) in faces:\n",
      "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
      "\n",
      "    # Display the resulting frame\n",
      "    cv2.imshow('Face Detection', frame)\n",
      "\n",
      "    # Terminate after 30 seconds\n",
      "    if (time.time() - start_time) > 30:\n",
      "        break\n",
      "\n",
      "    # Press 'q' to terminate early\n",
      "    if cv2.waitKey(1) == ord('q'):\n",
      "        break\n",
      "\n",
      "# Release the capture object and close the window\n",
      "cap.release()\n",
      "cv2.destroyAllWindows()\n",
      "```\n",
      "This code will open the default camera, detect faces, display them in real-time, and close after 30 seconds or when 'q' is pressed.Executing code...\n",
      "Code executed successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Below is a Python script using OpenCV to detect faces from the camera feed, display the output and terminate after 30 seconds:\\n\\n```python\\nimport cv2\\nimport time\\n\\n# Load a pre-trained face detection model\\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \\'haarcascade_frontalface_default.xml\\')\\n\\n# Initialize the video capture object\\ncap = cv2.VideoCapture(0)\\n\\n# Check if the camera opened successfully\\nif not cap.isOpened():\\n    print(\"Cannot open camera\")\\n    exit()\\n\\nstart_time = time.time()\\n\\nwhile True:\\n    # Read a frame from the camera\\n    ret, frame = cap.read()\\n    \\n    # If frame is read correctly, ret is True\\n    if not ret:\\n        print(\"Can\\'t receive frame (stream end?). Exiting ...\")\\n        break\\n\\n    # Convert the frame to grayscale (face detection is more efficient in grayscale)\\n    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\\n\\n    # Detect faces in the frame\\n    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\\n\\n    # Draw rectangles around the faces\\n    for (x, y, w, h) in faces:\\n        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\\n\\n    # Display the resulting frame\\n    cv2.imshow(\\'Face Detection\\', frame)\\n\\n    # Terminate after 30 seconds\\n    if (time.time() - start_time) > 30:\\n        break\\n\\n    # Press \\'q\\' to terminate early\\n    if cv2.waitKey(1) == ord(\\'q\\'):\\n        break\\n\\n# Release the capture object and close the window\\ncap.release()\\ncv2.destroyAllWindows()\\n```\\nThis code will open the default camera, detect faces, display them in real-time, and close after 30 seconds or when \\'q\\' is pressed.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "manager.run(\"Create a code for a face detector using my camera, show me it in display and it must run by 30 sec and end.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- RUN MANAGER -----------------\n",
      "User: Improve the display of the app with better visual design, such as counting faces and coloring the inner faces of the box and adding opacity to it.\n",
      "Manager: Improve the Python script for face detection to enhance the visual design by counting the number of faces, filling the detected faces with a semi-transparent colored box, and ensuring a good opacity level that allows the face to be visible underneath.\n",
      "\n",
      "ImproveDeveloper:\n",
      "To improve the face detection Python script, as you requested, you would need to:\n",
      "\n",
      "- Ensure that a face detection model or library such as OpenCV (cv2) is used.\n",
      "- Load the proper pre-trained face detection model.\n",
      "- Create a semi-transparent overlay to apply to the detected face regions.\n",
      "\n",
      "Let's start by ensuring the OpenCV library is correctly imported, then use the Haar Cascade Classifier for face detection.\n",
      "\n",
      "Below is an improved code snippet based on your requirements:\n",
      "\n",
      "```python\n",
      "import cv2\n",
      "import numpy as np\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "# Load pre-trained face detection model\n",
      "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
      "\n",
      "# Initialize the camera\n",
      "cap = cv2.VideoCapture(0)  \n",
      "\n",
      "# Set time to stop after 30 seconds\n",
      "stop_time = datetime.now() + timedelta(seconds=30)\n",
      "\n",
      "while True:\n",
      "    # Capture frame-by-frame\n",
      "    ret, frame = cap.read()\n",
      "\n",
      "    # Convert frame to grayscale\n",
      "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "    # Detect faces in the image\n",
      "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
      "\n",
      "    # Overlay settings\n",
      "    overlay = frame.copy()\n",
      "    opacity = 0.4\n",
      "    \n",
      "    # Font settings for the face count\n",
      "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
      "    font_scale = 1\n",
      "    font_color = (255, 255, 255)  # white\n",
      "    line_type = 2\n",
      "\n",
      "    # Draw rectangles around the faces\n",
      "    for (x, y, w, h) in faces:\n",
      "        # Apply semi-transparent overlay\n",
      "        cv2.rectangle(overlay, (x, y), (x+w, y+h), (255, 0, 0), -1)\n",
      "\n",
      "    # Apply the overlay with the specified alpha (opacity)\n",
      "    cv2.addWeighted(overlay, opacity, frame, 1 - opacity, 0, frame)\n",
      "\n",
      "    # Put the face count on the frame\n",
      "    cv2.putText(frame, f'Faces: {len(faces)}', (10, frame.shape[0] - 10), font, font_scale, font_color, line_type)\n",
      "\n",
      "    # Display the resulting frame\n",
      "    cv2.imshow('Face Detection', frame)\n",
      "\n",
      "    # Break from loop after 30 seconds\n",
      "    if datetime.now() > stop_time:\n",
      "        break\n",
      "\n",
      "    # Check for 'q' key to quit early\n",
      "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
      "        break\n",
      "\n",
      "# When everything is done, release the capture and destroy windows\n",
      "cap.release()\n",
      "cv2.destroyAllWindows()\n",
      "```\n",
      "\n",
      "Here are the improvements made to the original script:\n",
      "\n",
      "- The detection and overlay loop now includes an opacity-adjusted, colored rectangle to create the semi-transparent effect over each detected face.\n",
      "- The count of detected faces is displayed on the live video feed.\n",
      "- The program automatically terminates after 30 seconds, as per the initial requirement.\n",
      "- Face detection parameters are set to generally accepted starting values; these may need to be tuned for your specific use case.Executing code...\n",
      "Code executed successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"To improve the face detection Python script, as you requested, you would need to:\\n\\n- Ensure that a face detection model or library such as OpenCV (cv2) is used.\\n- Load the proper pre-trained face detection model.\\n- Create a semi-transparent overlay to apply to the detected face regions.\\n\\nLet's start by ensuring the OpenCV library is correctly imported, then use the Haar Cascade Classifier for face detection.\\n\\nBelow is an improved code snippet based on your requirements:\\n\\n```python\\nimport cv2\\nimport numpy as np\\nfrom datetime import datetime, timedelta\\n\\n# Load pre-trained face detection model\\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\\n\\n# Initialize the camera\\ncap = cv2.VideoCapture(0)  \\n\\n# Set time to stop after 30 seconds\\nstop_time = datetime.now() + timedelta(seconds=30)\\n\\nwhile True:\\n    # Capture frame-by-frame\\n    ret, frame = cap.read()\\n\\n    # Convert frame to grayscale\\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\\n\\n    # Detect faces in the image\\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\\n\\n    # Overlay settings\\n    overlay = frame.copy()\\n    opacity = 0.4\\n    \\n    # Font settings for the face count\\n    font = cv2.FONT_HERSHEY_SIMPLEX\\n    font_scale = 1\\n    font_color = (255, 255, 255)  # white\\n    line_type = 2\\n\\n    # Draw rectangles around the faces\\n    for (x, y, w, h) in faces:\\n        # Apply semi-transparent overlay\\n        cv2.rectangle(overlay, (x, y), (x+w, y+h), (255, 0, 0), -1)\\n\\n    # Apply the overlay with the specified alpha (opacity)\\n    cv2.addWeighted(overlay, opacity, frame, 1 - opacity, 0, frame)\\n\\n    # Put the face count on the frame\\n    cv2.putText(frame, f'Faces: {len(faces)}', (10, frame.shape[0] - 10), font, font_scale, font_color, line_type)\\n\\n    # Display the resulting frame\\n    cv2.imshow('Face Detection', frame)\\n\\n    # Break from loop after 30 seconds\\n    if datetime.now() > stop_time:\\n        break\\n\\n    # Check for 'q' key to quit early\\n    if cv2.waitKey(1) & 0xFF == ord('q'):\\n        break\\n\\n# When everything is done, release the capture and destroy windows\\ncap.release()\\ncv2.destroyAllWindows()\\n```\\n\\nHere are the improvements made to the original script:\\n\\n- The detection and overlay loop now includes an opacity-adjusted, colored rectangle to create the semi-transparent effect over each detected face.\\n- The count of detected faces is displayed on the live video feed.\\n- The program automatically terminates after 30 seconds, as per the initial requirement.\\n- Face detection parameters are set to generally accepted starting values; these may need to be tuned for your specific use case.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.run(\"Improve the display of the app with better visual design, such as counting faces and coloring the inner faces of the box and adding opacity to it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- RUN MANAGER -----------------\n",
      "User: All code before, document the code and explain it\n",
      "Manager: Document the provided improved Python script for face detection that enhances the visual design by counting the number of faces, filling the detected faces with a semi-transparent colored box, and ensuring a good opacity level.\n",
      "\n",
      "DocumentationDeveloper:\n",
      "As a code documenter, I would provide documentation based on the hypothetical Python script that has been described for face detection with visual enhancements. Since the actual code is not provided, I will describe the components and functionalities that such a script is expected to have, based on the features mentioned:\n",
      "\n",
      "### Script Overview\n",
      "This Python script is designed to detect faces in real-time using a webcam feed. It utilizes a pre-trained face detection model to locate faces within the video frames. Upon detection, the script counts the number of faces and overlays a semi-transparent colored box over each detected face.\n",
      "\n",
      "### Key Features\n",
      "1. **Face Detection**: Uses a computer vision library (e.g., OpenCV) to detect faces within the video stream from the webcam.\n",
      "2. **Face Count**: Keeps a running total of the detected faces in each frame.\n",
      "3. **Semi-Transparent Overlay**: Draws a colored box with set opacity around the detected faces without completely obscuring them, to allow for the facial features to be seen.\n",
      "4. **Timeout**: Automatically terminates the program after 30 seconds of runtime.\n",
      "\n",
      "### Visual Enhancements\n",
      "- **Colored Box**: The script draws a rectangle around the detected faces. The color and transparency level of the box are configurable, with the default being set to provide a clear view of the face.\n",
      "- **Opacity Level**: The filling opacity for the face detection box is carefully chosen to strike a balance between highlighting the detected face and maintaining visibility of the face's features.\n",
      "\n",
      "### Modules and Libraries\n",
      "- **OpenCV (cv2)**: Used for accessing the webcam and performing the face detection.\n",
      "- **Time**: For controlling the script's runtime duration.\n",
      "\n",
      "### Usage\n",
      "1. Ensure that all required libraries are installed.\n",
      "2. Run the script.\n",
      "3. Observe the video output with detected faces highlighted in real-time.\n",
      "4. After 30 seconds, the program will terminate automatically.\n",
      "\n",
      "### Restrictions\n",
      "- The face detection accuracy and performance might vary depending on the lighting conditions and the quality of the webcam.\n",
      "- The script is set to a fixed runtime of 30 seconds; this value is currently hardcoded and would need to be modified in the script if a different duration is desired.\n",
      "\n",
      "### Maintenance and Extensibility\n",
      "- Face detection parameters (such as the scale factor and minimum neighbors) can be adjusted to improve accuracy.\n",
      "- The color and opacity of the detection box can be parameterized if dynamic adjustments are required in the future.\n",
      "- The timeout duration could be modified, or code can be added to allow user input for more flexible operation.\n",
      "\n",
      "### Conclusion\n",
      "This script is a practical tool for face detection with visual enhancements that serve to provide both functional and aesthetic value. It is built with scalability in mind, allowing for further improvements and adaptations to meet the demands of various use cases.\n",
      "\n",
      "Please note that the actual Python code and detailed descriptions of the functions/methods used would be needed for specific documentation of codeblocks, classes, modules, and APIs.No code found to execute.\n",
      "\n",
      "DocumentationDeveloper:\n",
      "Acknowledged. As a Documentation Developer, my task is to document provided code to ensure clarity and understandability. If you have any actual code needing documentation or if there are specific aspects of the hypothetical face detection Python script that you would like to explore or document in more depth, please provide further details or the code itself, and I'll be happy to assist you with relevant documentation.No code found to execute.\n",
      "\n",
      "DocumentationDeveloper:\n",
      "Understood. As there is no code to document, I'm unable to generate specific documentation. If in the future you have a script or code that requires documentation, or if you need guidance on best practices for documenting code, please feel free to share the details or ask your questions, and I will provide assistance accordingly.No code found to execute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Understood. As there is no code to document, I'm unable to generate specific documentation. If in the future you have a script or code that requires documentation, or if you need guidance on best practices for documenting code, please feel free to share the details or ask your questions, and I will provide assistance accordingly.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.run(\"All code before, document the code and explain it\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws-multiagent-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
